{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "# Object Detection: TensorFlow vs OpenVINO\n",
    "We will use the following notebook to compare model performance on TensorFlow before and after conversion to Intermediate Representation (IR) with Model Optimizer and OpenVINO Toolkit. Some of the code on this notebook was reused and modified from Object Detection From TF1 Saved Model: https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/auto_examples/plot_object_detection_saved_model_tf1.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "# Object Detection with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object detection imports\n",
    "Here are the imports from the object detection module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, os.path\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from distutils.version import StrictVersion\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "import glob\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):\n",
    "  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wy72mWwAWKMK"
   },
   "source": [
    "## Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "v7m_NY_aWKMK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF 1.15.4\n"
     ]
    }
   ],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "print(\"TF\",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfn_tRFOWKMO"
   },
   "source": [
    "## Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "## Variables and Prepare Images \n",
    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.  \n",
    "\n",
    "In this app we will use \"SSD Lite with MobilenetV2 COCO\" model [here](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz). Other frozen topologies from TensorFlow supported by OpenVINO OpenVINO can be found in the [website](https://docs.openvinotoolkit.org/2019_R3.1/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html).\n",
    "\n",
    "In this section the image paths are prepared and imported for use with TensorFlow and reuse for OpenVINO Inference Engine once inference phase is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VyPz_t8WWKMQ"
   },
   "outputs": [],
   "source": [
    "# What model to use\n",
    "MODEL_NAME = '../model/ssdlite_mobilenet_v2_coco_2018_05_09'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBcB9QHLWKMU"
   },
   "source": [
    "## Load the frozen Tensorflow model into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KezjCRVvWKMV"
   },
   "outputs": [],
   "source": [
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "  od_graph_def = tf.GraphDef()\n",
    "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "    serialized_graph = fid.read()\n",
    "    od_graph_def.ParseFromString(serialized_graph)\n",
    "    tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aSlYc3JkWKMa"
   },
   "outputs": [],
   "source": [
    "# Function to load a frame into numpy array\n",
    "def load_image_into_numpy_array(image):\n",
    "  (im_width, im_height) = image.size\n",
    "  return np.array(image.getdata()).reshape(\n",
    "      (im_height, im_width, 3)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0_1AGhrWKMc"
   },
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jG-zn5ykWKMd"
   },
   "outputs": [],
   "source": [
    "# Two different paths used for data images, as we will inference on classified frames by a person (frames w/people, frames w/out people)\n",
    "PATH_TO_TEST_IMAGES_DIR = 'data/person/'\n",
    "TEST_IMAGE_PATHS = []\n",
    "valid_images = [\".jpg\"]\n",
    "for f in os.listdir(PATH_TO_TEST_IMAGES_DIR):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    TEST_IMAGE_PATHS.append(os.path.join(PATH_TO_TEST_IMAGES_DIR,f))\n",
    "\n",
    "PATH_TO_TEST_IMAGES_DIR2 = 'data/no-person/'\n",
    "TEST_IMAGE_PATHS2 = []\n",
    "valid_images = [\".jpg\"]\n",
    "for f in os.listdir(PATH_TO_TEST_IMAGES_DIR2):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    TEST_IMAGE_PATHS2.append(os.path.join(PATH_TO_TEST_IMAGES_DIR2,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "92BHxzcNWKMf"
   },
   "outputs": [],
   "source": [
    "inference_time_avg = 0\n",
    "def run_inference_for_single_image(IMAGE_PATHS):\n",
    "    itime = 0\n",
    "    start_inference = 0\n",
    "    graph = detection_graph\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            scores_people = []\n",
    "            duration = time.time()\n",
    "            inference_time = 0\n",
    "            inf_time = 0\n",
    "            for image_path in IMAGE_PATHS:\n",
    "                image = Image.open(image_path)\n",
    "                image_np = load_image_into_numpy_array(image)\n",
    "                # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "                image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "                # Get handles to input and output tensors\n",
    "                ops = tf.get_default_graph().get_operations()\n",
    "                all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "                tensor_dict = {}\n",
    "                for key in ['detection_scores']:\n",
    "                    tensor_name = key + ':0'\n",
    "                    if tensor_name in all_tensor_names:\n",
    "                        tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                        tensor_name)\n",
    "\n",
    "                image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "                expanded = np.expand_dims(image, 0)\n",
    "                # Run inference\n",
    "                start_inference = time.time()\n",
    "                output_dict = sess.run(tensor_dict,\n",
    "                                     feed_dict={image_tensor: expanded})\n",
    "                itime += (time.time() - start_inference)\n",
    "                # all outputs are float32 numpy arrays\n",
    "                output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "                scores_people.append(output_dict['detection_scores'][0])\n",
    "    return scores_people,itime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LQSEnEsPWKMj"
   },
   "outputs": [],
   "source": [
    "duration = time.time()\n",
    "scores_people,inf_time = run_inference_for_single_image(TEST_IMAGE_PATHS)\n",
    "duration = time.time() - duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration2 = time.time()\n",
    "scores_nopeople,inf_time2 = run_inference_for_single_image(TEST_IMAGE_PATHS2)\n",
    "duration2 = time.time() - duration2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_time_tot = inf_time + inf_time2\n",
    "inference_time_avg = ((inference_time_tot/(len(TEST_IMAGE_PATHS)+len(TEST_IMAGE_PATHS2)))/2)*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Frozen Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.10\n",
    "tp = 0 # true positives\n",
    "fn = 0 # false negatives\n",
    "for score in scores_people:\n",
    "    if(score >= confidence):\n",
    "        tp = tp + 1\n",
    "    if(score < confidence):\n",
    "        fn = fn + 1\n",
    "        \n",
    "tn = 0 #true negatives\n",
    "fp = 0 #false positives \n",
    "for score in scores_nopeople:\n",
    "    if(score >= confidence):\n",
    "        fp = fp + 1\n",
    "    if(score < confidence):\n",
    "        tn = tn + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = np.array(scores_people)\n",
    "people_frames = np.array(TEST_IMAGE_PATHS) \n",
    "nopeople = np.array(scores_nopeople)\n",
    "nopeople_frames = np.array(TEST_IMAGE_PATHS2) \n",
    "\n",
    "output = np.column_stack((people_frames,people))\n",
    "output2 = np.column_stack((nopeople_frames,nopeople))\n",
    "\n",
    "np.savetxt(\"results/people-results-tf.csv\", output, delimiter=',', header=\"Frame,Score\", fmt='%s')\n",
    "np.savetxt(\"results/no-people-results-tf.csv\", output2, delimiter=',', header=\"Frame,Score\", fmt='%s')\n",
    "\n",
    "test = len(TEST_IMAGE_PATHS) + len(TEST_IMAGE_PATHS2)\n",
    "accuracy = ((tp + tn) / test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of TensorFlow Frozen Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time:\t 36.45 sec\n",
      "Avg. Inference:\t 13.07 ms\n",
      "Accuracy:\t 92.04% \n",
      "FPS:\t\t 4.79\n"
     ]
    }
   ],
   "source": [
    "#print(\"TP: {}  |  FN: {}  |  FP: {}  |  TN: {}\".format(tp,fn,fp,tn))\n",
    "print(\"Inference Time:\\t {:.02f} sec\".format(inference_time_tot))\n",
    "print(\"Avg. Inference:\\t {:.02f} ms\".format((inference_time_avg)))\n",
    "print(\"Accuracy:\\t {:.2f}% \\nFPS:\\t\\t {:.2f}\".format(accuracy,test/(duration + duration2 - inference_time_tot)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser, SUPPRESS\n",
    "import cv2\n",
    "import time\n",
    "import logging as log\n",
    "from openvino.inference_engine import IENetwork, IEPlugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(image_paths, device):\n",
    "    try:\n",
    "        log.basicConfig(format=\"[ %(levelname)s ] %(message)s\", level=log.INFO, stream=sys.stdout)\n",
    "        \n",
    "        # variables\n",
    "        avg_inf_time = 0\n",
    "        cpu_extension = \"/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/libcpu_extension_avx2.so\"\n",
    "        device = device\n",
    "        frames_processed = 0\n",
    "        model_path = \"../model/ssdlite_mobilenet_v2_coco_2018_05_09/ssdlite_mobilenet_v2_coco.xml\"\n",
    "        model_xml = model_path\n",
    "        model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "        plugin_dir = None\n",
    "        prob_threshold = 0.0\n",
    "        \n",
    "        # Plugin initialization for specified device and load extensions library if specified\n",
    "        log.info(\"Initializing plugin for {} device...\".format(device))\n",
    "        plugin = IEPlugin(device=device, plugin_dirs=plugin_dir)\n",
    "        \n",
    "        if cpu_extension and device == 'CPU':\n",
    "            plugin.add_cpu_extension(cpu_extension)\n",
    "        # Read IR\n",
    "        log.info(\"Reading IR...\")\n",
    "        net = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "        if plugin.device == \"CPU\":\n",
    "            supported_layers = plugin.get_supported_layers(net)\n",
    "            not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "            if len(not_supported_layers) != 0:\n",
    "                log.error(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                          format(plugin.device, ', '.join(not_supported_layers)))\n",
    "                log.error(\"Please try to specify cpu extensions library path in demo's command line parameters using -l \"\n",
    "                          \"or --cpu_extension command line argument\")\n",
    "                sys.exit(1)\n",
    "                \n",
    "        assert len(net.inputs.keys()) == 1, \"App supports only single input topologies\"\n",
    "        assert len(net.outputs) == 1, \"App supports only single output topologies\"\n",
    "        input_blob = next(iter(net.inputs))\n",
    "        out_blob = next(iter(net.outputs))\n",
    "        log.info(\"Loading IR to the plugin...\")\n",
    "        exec_net = plugin.load(network=net, num_requests=2)\n",
    "        # Read and pre-process input image\n",
    "        n, c, h, w = net.inputs[input_blob].shape\n",
    "        del net\n",
    "        total_inf_start = time.time()\n",
    "        inf_scores = []\n",
    "        log.info(\"Starting inference requests in sync mode...\")\n",
    "        for image_path in image_paths:\n",
    "            \n",
    "            input_stream = image_path\n",
    "            assert os.path.isfile(image_path), \"Specified input file doesn't exist\"\n",
    "            \n",
    "            cap = cv2.VideoCapture(input_stream)\n",
    "\n",
    "            cur_request_id = 0\n",
    "\n",
    "            render_time = 0\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                initial_w = cap.get(3)\n",
    "                initial_h = cap.get(4)\n",
    "\n",
    "                in_frame = cv2.resize(frame, (w, h))\n",
    "                in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "                in_frame = in_frame.reshape((n, c, h, w))\n",
    "                inf_start = time.time()\n",
    "                exec_net.start_async(request_id=cur_request_id, inputs={input_blob: in_frame})\n",
    "                if exec_net.requests[cur_request_id].wait(-1) == 0:\n",
    "\n",
    "                    # Parse detection results of the current request\n",
    "                    res = exec_net.requests[cur_request_id].outputs[out_blob]\n",
    "                    # Append score of inference request result\n",
    "                    inf_scores.append(res[0][0][0,2])\n",
    "                    avg_inf_time += (time.time() - inf_start)\n",
    "        \n",
    "        log.info(\"Inference requests completed\")\n",
    "        \n",
    "        # Report performance stats\n",
    "        total_inf_end = time.time()\n",
    "        total_det_time = total_inf_end - total_inf_start\n",
    "        inf_time_message = \"Total detection time on {}: {:.3f} sec\".format(device,total_det_time)\n",
    "        print(inf_time_message)\n",
    "        print(\"Avg. inference time: {} ms\".format((avg_inf_time/len(image_paths))*1000))\n",
    "        \n",
    "        return inf_scores,total_det_time,avg_inf_time;\n",
    "\n",
    "    except ValueError: \n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference with Inference Engine\n",
    "Here we begin inferencing with device of choice. Possible device options if present on system are:\n",
    "{CPU, GPU, MYRIAD, HDDL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person data baseline: \n",
      "[ INFO ] Initializing plugin for CPU device...\n",
      "[ INFO ] Reading IR...\n",
      "[ INFO ] Loading IR to the plugin...\n",
      "[ INFO ] Starting inference requests in sync mode...\n",
      "[ INFO ] Inference requests completed\n",
      "Total detection time on CPU: 10.574 sec\n",
      "Avg. inference time: 6.987771987915039 ms\n",
      "\n",
      "no-person data baseline:\n",
      "[ INFO ] Initializing plugin for CPU device...\n",
      "[ INFO ] Reading IR...\n",
      "[ INFO ] Loading IR to the plugin...\n",
      "[ INFO ] Starting inference requests in sync mode...\n",
      "[ INFO ] Inference requests completed\n",
      "Total detection time on CPU: 3.493 sec\n",
      "Avg. inference time: 7.1150015952975245 ms\n"
     ]
    }
   ],
   "source": [
    "device = \"CPU\"\n",
    "print(\"person data baseline: \")\n",
    "scores_people2, detection_time3, avg_inf3 = run_inference(TEST_IMAGE_PATHS, device)\n",
    "print(\"\\nno-person data baseline:\")\n",
    "scores_nopeople2, detection_time4, avg_inf4 = run_inference(TEST_IMAGE_PATHS2, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Optimized Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp2 = 0 # true positives\n",
    "fn2 = 0 # false negatives\n",
    "for score in scores_people2:\n",
    "    if(score >= confidence):\n",
    "        tp2 = tp2 + 1\n",
    "    if(score < confidence):\n",
    "        fn2 = fn2 + 1\n",
    "        \n",
    "tn2 = 0 #true negatives\n",
    "fp2 = 0 #false positives \n",
    "for score in scores_nopeople2:\n",
    "    if(score >= confidence): # Threshold of .25\n",
    "        fp2 = fp2 + 1\n",
    "    if(score < confidence):\n",
    "        tn2 = tn2 + 1\n",
    "        \n",
    "\n",
    "people2 = np.array(scores_people2)\n",
    "people_frames2 = np.array(TEST_IMAGE_PATHS) \n",
    "nopeople2 = np.array(scores_nopeople2)\n",
    "nopeople_frames2 = np.array(TEST_IMAGE_PATHS2) \n",
    "test2 = len(TEST_IMAGE_PATHS) + len(TEST_IMAGE_PATHS2)\n",
    "accuracy2 = ((tp2 + tn2) / test2)*100\n",
    "\n",
    "output3 = np.column_stack((people_frames2,people2))\n",
    "output4 = np.column_stack((nopeople_frames2,nopeople2))\n",
    "\n",
    "np.savetxt(\"results/people-results-openvino-cpu.csv\", output3, delimiter=',', header=\"Frame,Score\", fmt='%s')\n",
    "np.savetxt(\"results/no-people-results-openvino-cpu.csv\", output4, delimiter=',', header=\"Frame,Score\", fmt='%s')\n",
    "\n",
    "inference_time_avg2 = (avg_inf3 + avg_inf4)/2\n",
    "\n",
    "fps = test/(duration + duration2 - inference_time_tot)\n",
    "fps2 = test2/(detection_time3 + detection_time4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Optimized Model with OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time:\t 14.07 sec\n",
      "Avg. Inference:  4.89 ms\n",
      "Accuracy:\t 88.95% \n",
      "FPS:\t\t 99.09\n"
     ]
    }
   ],
   "source": [
    "#print(\"TP: {}  |  FN: {}  |  FP: {}  |  TN: {}\".format(tp,fn,fp,tn))\n",
    "print(\"Inference Time:\\t {:.02f} sec\\nAvg. Inference:  {:.2f} ms\".format((detection_time3 + detection_time4),(avg_inf3 + avg_inf4)/2))   \n",
    "print(\"Accuracy:\\t {:.2f}% \\nFPS:\\t\\t {:.2f}\".format((accuracy2),test2/(detection_time3 + detection_time4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Model and Intemmediate Representation File Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model File Size (MB):\t18.99\t17.19\t9.46\n"
     ]
    }
   ],
   "source": [
    "# Get model file sizes\n",
    "file_size_pb = 0\n",
    "file_size_xml = 0\n",
    "file_size_bin = 0\n",
    "file_stats = os.stat(\"../model/ssdlite_mobilenet_v2_coco_2018_05_09/frozen_inference_graph.pb\")\n",
    "file_size_pb = file_stats.st_size / (1024 * 1024)\n",
    "file_stats = os.stat(\"../model/ssdlite_mobilenet_v2_coco_2018_05_09/ssdlite_mobilenet_v2_coco.xml\")\n",
    "file_size_xml = file_stats.st_size / (1024 * 1024)\n",
    "file_stats = os.stat(\"../model/ssdlite_mobilenet_v2_coco_2018_05_09/ssdlite_mobilenet_v2_coco.bin\")\n",
    "file_size_bin = file_stats.st_size / (1024 * 1024)\n",
    "print(\"Model File Size (MB):\\t{:.02f}\\t{:.02f}\\t{:.02f}\".format(file_size_pb,file_size_xml + file_size_bin, ((file_size_xml + file_size_bin-file_size_pb)/(file_size_pb)*-100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_headers = \"Metric\\tTensorFlow\\tOpenVINO\\t%Difference\"\n",
    "res_inf = \"Inference Time (s):\\t{:.02f}\\t{:.02f}\\t{:.02f}\".format((inference_time_tot),(detection_time3 + detection_time4),(((detection_time3 + detection_time4)-inference_time_tot)/(inference_time_tot)*-100))\n",
    "res_inf2 = \"Avg. Inference (ms):\\t{:.02f}\\t{:.02f}\\t{:.02f}\".format(inference_time_avg,inference_time_avg2,((inference_time_avg2-inference_time_avg)/(inference_time_avg)*-100))\n",
    "res_acc = \"Accuracy (%):\\t{:.02f}\\t{:.02f}\\t{:.02f}\".format(accuracy,accuracy2,((accuracy2-accuracy)/accuracy*-100))\n",
    "res_fps = \"FPS:\\t{:.2f}\\t{:.2f}\\t{:.02f}\".format(fps,fps2,((fps2-fps)/fps)*100)\n",
    "res_size = \"Model File Size (MB):\\t{:.02f}\\t{:.02f}\\t{:.02f}\".format(file_size_pb,file_size_xml + file_size_bin, ((file_size_xml + file_size_bin-file_size_pb)/(file_size_pb)*-100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find results stored in /home/leavitia/udacity/people-counter-app/resources/tf-openvino-metrics.csv\n"
     ]
    }
   ],
   "source": [
    "res_inf = np.array(res_inf)\n",
    "res_inf2 = np.array(res_inf2)\n",
    "res_acc = np.array(res_acc)\n",
    "res_fps = np.array(res_fps)\n",
    "res_size = np.array(res_size)\n",
    "\n",
    "output = np.row_stack((res_inf,res_inf2,res_acc,res_fps,res_size))\n",
    "\n",
    "np.savetxt(\"results/tf-openvino-metrics.csv\", output, delimiter=',', header=res_headers, fmt='%s')\n",
    "dir_path = os.getcwd()\n",
    "print(\"Find results stored in {}/tf-openvino-metrics.csv\".format(dir_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric\tTensorFlow\tOpenVINO\t%Difference \n",
      " Inference Time (s):\t36.45\t14.07\t61.41 \n",
      " Avg. Inference (ms):\t13.07\t4.89\t62.58 \n",
      " Accuracy (%):\t92.04\t88.95\t3.35 \n",
      " FPS:\t4.79\t99.09\t1970.12 \n",
      " Model File Size (MB):\t18.99\t17.19\t9.46\n"
     ]
    }
   ],
   "source": [
    "print(res_headers,\"\\n\",res_inf,\"\\n\",res_inf2,\"\\n\",res_acc,\"\\n\",res_fps,\"\\n\",res_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
